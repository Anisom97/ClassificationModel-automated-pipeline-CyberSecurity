{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_plot(inp_df,inp_feature_count,inp_model_name):\n",
    "    scaler=MinMaxScaler()\n",
    "    inp_df.columns=['Features','Importance']\n",
    "    importance_scaled=pd.DataFrame(scaler.fit_transform(inp_df[['Importance']].values),index=inp_df.index, columns=['Importance'])\n",
    "    inp_df['Importance']=importance_scaled['Importance']\n",
    "    inp_df.sort_values(by='Importance', inplace=True, ascending=True)\n",
    "    df_plot=inp_df.tail(inp_feature_count).reset_index(drop=True)\n",
    "    fig=go.Figure()\n",
    "    fig.add_trace(go.Bar(x=df_plot['Importance'],\n",
    "                        y=df_plot['Features']\n",
    "                        orientation='h'))\n",
    "    fig.update_traces(marker_color='steelblue')\n",
    "    fig.update_traces(title='Importance')\n",
    "    fig.update_traces(title='Features')\n",
    "    fig.update_traces(title='Feature selection using {}'.format(inp_model_name))\n",
    "    fig.show(config={'displaylogo':False})\n",
    "    return inp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_plot(cross_val_scores,model_name,metric_name):\n",
    "    abs_error=[abs(ele) for ele in cross_val_scores]\n",
    "    folds=[\"Fold {}\".format(ele) for ele in range(1,len(cross_val_scores)+1)]\n",
    "    fig=go.Figure([go.Bar(x=folds,y=abs_error,marker_color='steelblue')])\n",
    "    title='{} fold cross-validation for {}'.format(len(cross_val_scores),model_name)\n",
    "    fig.update_layout(title=title,\n",
    "                     xaxis_title='Folds'\n",
    "                     yaxis_title=metric_name, width=900,height=500)\n",
    "    fig.show(config={'displaylogo':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_result(classifier,x_data_t,y_data_t,cv_k,params):\n",
    "    skf=StratifiedKFold(n_splits=cv_k,shuffle=True,random_state=369)\n",
    "    grid_search=GridSearchCV(classifier,param_grid=params,\n",
    "                            cv=skf.split(x_data_t,y_data_t),\n",
    "                            scoring='f1_macro',n_jobs=-1,verbose=3)\n",
    "    model_opt=grid_searchd_search.fit(x_data_t,y_data_t)\n",
    "    \n",
    "    cv_res_df=pd.DataFrame(data=model_opt.cv_results_)\n",
    "    cv_res_df.drop(['mean_fit_time','std_fit_time','mean_score_time','std_test_score','params'],axis=1,inplace=True)\n",
    "    cv_res_df_col=list(cv_res_df.columns)\n",
    "    cv_res_df_col.insert(0,cv_res_df_col.pop())\n",
    "    cv_res_df=cv_res_df.reindex(columns=cv_res_df_col).sort_values(by='rank_test_score')\n",
    "    \n",
    "    opt_params=model_opt.best_params_\n",
    "    classifier=classifier.set_params(**opt_params)\n",
    "    final_model=classifier.fit(x_data_t,y_data_t)\n",
    "    y_pred_t=final_model.predict(x_data_t)\n",
    "    cv_scores=cross_valid_score(classifier,x_data_t,y_data_t,cv=cv_k,scoring='f1_macro')\n",
    "    clear_output()\n",
    "    \n",
    "    display(Markdown('__Cross validation for TRAIN dataset__'))\n",
    "    display_data(cv_res_df.round(4))\n",
    "    \n",
    "    display(Markdown('__Best parameters__'))\n",
    "    for k,v in opt_params.items():\n",
    "        display(Markdown('__{}__ : {}'.format(k,v)))\n",
    "        \n",
    "    return final_model,cv_scores,y_pred_t,opt_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cm(cm):\n",
    "    FP=cm.sum(axis=0)-np.diag(cm)\n",
    "    FN=cm.sum(axis=1)-np.diag(cm)\n",
    "    TP=np.diag(cm)\n",
    "    TN=cm.sum()-(FP+FN+TP)\n",
    "    \n",
    "    FP=FP.astype(float)\n",
    "    FN=FN.astype(float)\n",
    "    TP=TP.astype(float)\n",
    "    TN=TN.astype(float)\n",
    "    \n",
    "    return FP,FN,TP,TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metric_calculation(y_train,y_pred_train,x_valid,y_valid,model,model_name):\n",
    "    classes=y_train.value_counts().index.to_list()\n",
    "    \n",
    "    acc_train=accuracy_scores(y_train,y_pred_train)*100\n",
    "    class_report_train=classification_report(y_train,y_pred_train,\n",
    "                                            target_names=classes,output_dict=True)\n",
    "    \n",
    "    y_pred_valid=model.predict(x_valid)\n",
    "    acc_valid=accuracy_score(y_valid,y_pred_valid)*100\n",
    "    class_report_train=classification_report(y_valid,y_pred_valid,\n",
    "                                            target_names=classes,output_dict=True)\n",
    "    \n",
    "    kappa_train=cohen_kappa_score(y_train,y_pred_train)\n",
    "    kappa_valid=cohen_kappa_score(y_valid,y_pred_valid)\n",
    "    \n",
    "    display(Markdown('***'))\n",
    "    display(Markdown('__Overall statistics__'))\n",
    "    display(Markdown('Accuracy on TRAIN DATA: {}%'.format(round(acc_train,4))))\n",
    "    display(Markdown('Accuracy on VALIDATION DATA: {}%'.format(round(acc_valid,4))))\n",
    "    \n",
    "    display(Markdown('Kappa score on TRAIN DATA: {}%'.format(round(kappa_train,4))))\n",
    "    display(Markdown('Kappa score on VALIDATION DATA: {}%'.format(round(kappa_valid,4))))\n",
    "    display(Markdown('***'))\n",
    "\n",
    "    cr_df_train=pd.DataFrame(class_report_train).T\n",
    "    cr_df_valid=pd.DataFrame(class_report_valid).T\n",
    "    \n",
    "    train_cr_metric=cr_df_train[:len(classes)]\n",
    "    valid_cr_metric=cr_df_valid[:len(classes)]\n",
    "    \n",
    "    train_cr_metric=train_cr_metric.iloc[:,:-1]\n",
    "    valid_cr_metric=valid_cr_metric.iloc[:,:-1]\n",
    "    \n",
    "    cm_train=confusion_matrix(y_train,y_pred_train)\n",
    "    cm_valid=confusion_matrix(y_valid,y_pred_valid)\n",
    "    \n",
    "    fp_t,fn_t,tp_t,tn_t=calculate_cm(cm_train)\n",
    "    fp_v,fn_v,tp_v,tn_v=calculate_cm(cm_valid)\n",
    "    \n",
    "    train_spcfcty=tn_t/(tn_t+fp_t)\n",
    "    valid_spcfcty=tn_v/(tn_v+fp_v)\n",
    "    \n",
    "    train_cr_metric['specificity']=list(train_spcfcty)\n",
    "    valid_cr_metric['specificity']=list(valid_spcfcty)\n",
    "    \n",
    "    train_prev=(tp_t+fn_t)/(tp_t+fp_t+tn_t+fn_t)\n",
    "    valid_prev=(tp_v+fn_v)/(tp_v+fp_v+tn_v+fn_v)\n",
    "    \n",
    "    train_cr_metric['prevalence']=list(train_prev)\n",
    "    valid_cr_metric['prevalence']=list(valid_prev)\n",
    "    \n",
    "    train_b_acc=[sum(i)/2 for i in zip(list(train_cr_metric['recall']),list(train_spcfcty))]\n",
    "    valid_b_acc=[sum(i)/2 for i in zip(list(valid_cr_metric['recall']),list(valid_spcfcty))]\n",
    "    \n",
    "    train_cr_metric['balanced accuracy']=train_b_acc\n",
    "    valid_cr_metric['balanced accuracy']=valid_b_acc\n",
    "    \n",
    "    train_ppv=tp_t/(tp_t+fp_t)\n",
    "    valid_ppv=tp_v/(tp_v+fp_v)\n",
    "    \n",
    "    train_cr_metric['PPV']=list(train_ppv)\n",
    "    valid_cr_metric['PPV']=list(valid_ppv)\n",
    "    \n",
    "    train_npv=tn_t/(tn_t+fn_t)\n",
    "    valid_npv=tn_v/(tn_v+fn_v)\n",
    "    \n",
    "    train_cr_metric['NPV']=list(train_npv)\n",
    "    valid_cr_metric['NPV']=list(valid_npv)\n",
    "    \n",
    "    train_det_rate=tp_t/(tp_t+fp_t+tn_t+fn_t)\n",
    "    valid_det_rate=tp_v/(tp_v+fp_v+tn_v+fn_v)\n",
    "    \n",
    "    train_cr_metric['detection rate']=list(train_det_rate)\n",
    "    valid_cr_metric['detection rate']=list(valid_det_rate)\n",
    "    \n",
    "    train_det_prev=(tp_t+fp_t)/(tp_t+fp_t+tn_t+fn_t)\n",
    "    valid_det_prev=(tp_v+fp_v)/(tp_v+fp_v+tn_v+fn_v)\n",
    "    \n",
    "    train_cr_metric['detection prevalence']=list(train_det_prev)\n",
    "    valid_cr_metric['detection_prevalence']=list(valid_det_prev)\n",
    "    \n",
    "    train_cr_metric=train_cr_metric.T\n",
    "    valid_cr_metric=valid_cr_metric.T\n",
    "    \n",
    "    train_cr_metric.insert(loc=0, column='metric', value=train_cr_metric.index)\n",
    "    valid_cr_metric.insert(loc=0, column='metric', value=valid_cr_metric.index)\n",
    "    \n",
    "    display(Markdown('_Performance Metrics of **{}** for different classes: **TRAIN DATA**_'.format(model_name)))\n",
    "    display_data(train_cr_metric.round(4))\n",
    "    display(Markdown('_Performance Metrics of **{}** for different classes: **VALIDATION DATA**_'.format(model_name)))\n",
    "    display_data(valid_cr_metric.round(4))\n",
    "    \n",
    "    train_avg_metric=cr_dt_train[len(classes)+1:].T\n",
    "    valid_avg_metric=cr_dt_valid[len(classes)+1:].T\n",
    "    \n",
    "    train_avg_metric['micro_avg']=precision_recall_fscore_support(y_train,\n",
    "                                                                 y_pred_train, average='micro')\n",
    "    valid_avg_metric['macro_avg']=precision_recall_fscore_support(y_valid,\n",
    "                                                                 y_pred_valid, average='macro')\n",
    "    \n",
    "    train_avg_metric=train_avg_metric.iloc[:-1,:]\n",
    "    valid_avg_metric=valid_avg_metric.iloc[:-1,:]\n",
    "    \n",
    "    display(Markdown('_Performance Metrics of **{}** for different classes: **TRAIN DATA**_'.format(model_name)))\n",
    "    display_data(train_avg_metric)\n",
    "    display(Markdown('_Performance Metrics of **{}** for different classes: **VALIDATION DATA**_'.format(model_name)))\n",
    "    display_data(valid_avg_metric)\n",
    "    \n",
    "    cm_fig=ff.create_annotated_heatmap(cm_train,x=classes,y=classes, colorscale='darkmint',showscale=True)\n",
    "    cm_fig.update_layout(width=800, height=800, title='Confusion Matrix',xaxis_title=\"Predicted\",yaxis_title=\"Reference\")\n",
    "    display(cm_fig)\n",
    "    return train_avg_metric,valid_avg_metric,y_pred_valid,acc_train,acc_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data(data_table,sek_col=None):\n",
    "    data_table_series=[data_table[i] for i in data_table.columns]\n",
    "    \n",
    "    cell_color=[]\n",
    "    n=len(data_table)\n",
    "    for col in data_table.columns;\n",
    "        if sel_col is None:\n",
    "            cell_color.append(['mintcream']*n)\n",
    "            else:\n",
    "                if col!=sel_col:\n",
    "                    cell_color.append(['mintcream']*n)\n",
    "                else:\n",
    "                    cell_color.append(['lightgreen']*n)\n",
    "    fig=go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(data_table.columns),\n",
    "                   fill_color='darkslategrey',\n",
    "                   align='center',\n",
    "                   font=dict(color='white', size=15)\n",
    "                   ),\n",
    "        cells=dict(values=data_table_series,\n",
    "                  ill_color='darkslategrey',\n",
    "                   align='center',\n",
    "                   font=dict(color='black', size=11)\n",
    "                   ))]) \n",
    "    \n",
    "    if data_table.shape[0]==1:\n",
    "        fig_ht=75\n",
    "    elif data_table.shape[0]>=2 and data_table.shape[0]<=9:\n",
    "        fig_ht=30*data_table.shape[0]\n",
    "    else:\n",
    "        fig_ht=300\n",
    "        \n",
    "    fig.update_layout(width=150*len(data_table.columns),\n",
    "                     height=fig_ht,\n",
    "                     margin=dict(l=0,r=0,b=0,t=0,pad=0))\n",
    "    display(fig)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_var_set(df):\n",
    "    if df[m.target_var].dtypes!=object:\n",
    "        df[m.target_var]=df[ma.target_var].astype('str')\n",
    "        \n",
    "    print(colored(\"Selected target variable is:\",'magenta',attrs=['bold']),colored(\"{}\",'blue',attrs=['bold']).format(m.target_var))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encoding(df):\n",
    "    categorical_var=df.filter(m.encode_cols)\n",
    "    one_hot_categorical_var=pd.get_dummies(categorical_var,columns=m.encode_cols\n",
    "                                          ,drop_first=True)\n",
    "    lr_data_df1=pd.concat([df,one_hot_categorical],axis=1,sort=False)\n",
    "    lr_data_df1=lr_data_df1.drop(m.encode_cols,axis=1)\n",
    "    \n",
    "    df=lr_data_df1.copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df,df_cp):\n",
    "    split_percentage=m.split_amt\n",
    "    cv_method=m.cv_input\n",
    "    cv_k=m.cv_value\n",
    "    \n",
    "    df_transformed=df.copy()\n",
    "    split_percentage=split_percentage/100\n",
    "    X=df_transformed.loc[:,df.columns]\n",
    "    y=df_transformed[m.target_var]\n",
    "    X=X.drop(m.target_var,axis=1)\n",
    "    \n",
    "    x_train,x_valid,y_train,y_valid=train_test_split(X,y,test_size=split_percentage,\n",
    "                                                    random_state=0,stratify=y)\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Training-Validation Split Percentage:{split}\".format(split=int((1-split_percentage)*100)))\n",
    "    print(\"Total Observation:{obs}\".format(obs=X.shape[0]))\n",
    "    print(\"Training Observation:{train_obs}\".format(train_obs=x_train.shape[0]))\n",
    "    print(\"Validation Observation:{valid_obs}\".format(valid_obs=x_valid.shape[0]))\n",
    "    print(\"Selected k cross validation splits is:{cv_k}\".format(cv_k=cv_k))\n",
    "    \n",
    "    il_valid=x_valid.index.tolist()\n",
    "    x_valid_cp=df_cp.iloc[il_valid]\n",
    "    return x_train,x_valid,y_train,y_valid,x_valid_cp,cv_k,cv_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_dependency(x_train):\n",
    "    feature_space=x_train.loc[:,x_train.columns!=m.target_var]\n",
    "    reduced_form,inds=sympy.Matrix(feature_space.values).rref()\n",
    "    indep_features=[feature_space.columns.tolist()[i] for i in inds]\n",
    "    collinear_features=[item for item in feature_space.columns if item not in set(indep_features)]\n",
    "    \n",
    "    if len(collinear_features)>0:\n",
    "            print(\"\\nFeatures causing linear dependency:{ld}\".format(ld=\" || \".join(collinear_features)))\n",
    "            print(\"These variables will be removed which are causing linear dependency\")\n",
    "            x_train_processed=x_train[indep_features]\n",
    "            print(\"Data contains {} rows and {} columns\".format(x_train_processed.shape[0],x_train_processed_shape[1]))\n",
    "            display(x_train_processed.dtypes)\n",
    "    else:\n",
    "        print(\"No linear dependent columns\")\n",
    "        x_train_processed=x_train.copy()\n",
    "    \n",
    "    return x_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_type_sep(df):\n",
    "    cal_l=df.select_dtypes(exclude=[\"float64\",\"int64\"]).columns.tolist()\n",
    "    num_l=df.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "    col_drop=[]\n",
    "    if m.target_var in cat_l:\n",
    "        cat_l.remove(m.target_var)\n",
    "    else:\n",
    "        num_l.remove(m.target_var)\n",
    "        \n",
    "    return cat_l,num_l,col_drop\n",
    "\n",
    "\n",
    "def lin_dep(cat_l,num_l,df,col_drop):\n",
    "    for i in cat_l:\n",
    "        CategoryGroupLists=df.groupby(i)[m.target_var].apply(list)\n",
    "        AnnovaResults=f_oneway(*CategoryGroupLists)\n",
    "        if(AnnovaResults[1]>0.05):\n",
    "            col_drop.append(i)\n",
    "    for i in num_l:\n",
    "        cr=df[i].corr(df[m.target_var])\n",
    "        if(cr==0):\n",
    "            col_drop.append(i)\n",
    "            \n",
    "    return col_drop\n",
    "\n",
    "\n",
    "def variability(cat_l,num_l,col_drop):\n",
    "    for i in num_l:\n",
    "        if(df[i].describe().loc['std']==0):\n",
    "            col_drop_append(i)\n",
    "    return col_drop\n",
    "\n",
    "def preprocess_df(col_drop,df):\n",
    "    df=df[df.columns.difference(col_drop)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_features(x_train_processed_balanced):\n",
    "    feature_count=10\n",
    "    \n",
    "    selected_model_features={}\n",
    "    \n",
    "    X_train_selection=x_train_processed_balanced.copy()\n",
    "    return X_train_selection,selected_model_features,feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_pls(x_train_processed_balanced,y_train_processed_balanced,selected_model_features,feature_count):\n",
    "    feature_selection_pls='Y'\n",
    "    if feature_selection_pls=='Y':\n",
    "        X=x_train_processed_balanced.copy()\n",
    "        y=y_train_processed_balanced.copy()\n",
    "        \n",
    "        clear_output()\n",
    "        pls=PLSRegression(n_components=X.shape[1])\n",
    "        le=LabelEncoder()\n",
    "        y_=le.fit_transform(y)\n",
    "        pls.fit(X,y_)\n",
    "        \n",
    "        minmax=MinMaxScaler()\n",
    "        coeff=abs(pls.coef_)\n",
    "        scaled_coeff=pd.DataFrame(minmax.fit_transform(coeff))*100\n",
    "        \n",
    "        df_plot_pls=pd.concat([pd.DataFrame(X.columns),scaled_coeff],axis=1)\n",
    "        model_name=\"PLS\"\n",
    "        \n",
    "        df_plot_pls=feature_selection_plot(df_plot_pls,feature_count,model_name)\n",
    "        \n",
    "        df_pls=df_plot_pls.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "        display(df_pls[['Features','Importance']].head(feature_count).round(4))\n",
    "        selected_model_features.update({\"PLS\":df_pls.Features.tolist()[:feature_count]})\n",
    "    else:\n",
    "        clear_output()\n",
    "        selected_model_features.pop(\"PLS\",0)\n",
    "        \n",
    "    return selected_model_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_feature_set(x_train_processed_balanced,selected_model_features):\n",
    "    feature_selection_mode=\"U\"\n",
    "    add_features='N'\n",
    "    col_add=[]\n",
    "    \n",
    "    if feature_selection_mode=='I':\n",
    "        selected_features=list(reduce(set.intersection,(set(val) for val in selected_model_features.values())))\n",
    "        if len(selected_features)==0:\n",
    "            display(Markdown(__No common features__))\n",
    "    else:\n",
    "        selected_features=list(reduce(set.union,(set(val) for val in selected_model_features.values())))\n",
    "        \n",
    "    if add_feature=='Y':\n",
    "        selected_features.extend(col_add)\n",
    "        selected_features=list(set(selected_features))\n",
    "        \n",
    "    clear_output()\n",
    "    if len(selected_features)==0:\n",
    "        display(Markdown(\"No feature selected\"))\n",
    "        x_train_processed_balanced_subset=x_train_processed_balanced.copy()\n",
    "    else:\n",
    "        display(Markdown(\"__Features selected for modelling__\"))\n",
    "        display(pd.DataFrame(selected_features,columns=['Features']))\n",
    "        final_features=selected_features\n",
    "        x_train_processed_balanced_subset=x_train_processed_balanced[x_train_processed_balanced.columns.intersection(selected_features)]\n",
    "    \n",
    "    return x_train_processed_balanced_subset,selected_features,final_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
